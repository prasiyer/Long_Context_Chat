{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of experiments to run\n",
    "## 1] Default scenario\n",
    "## 2] flashattention\n",
    "## 3] expandable_segments: Set expandable segments -- check if it works in notebook\n",
    "## 4] flash_expandable: flashattention + expandable segments\n",
    "## 5] torch.compile\n",
    "## 6] sdpa\n",
    "## 7] sdpa_torch.compile: sdpa + torch.compile\n",
    "## 8] 8_bit: 8-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_for_experiment = [\"What is the firm period for Ognibene?\", \"What is the firm period for Hengli?\", \"What are the material planning requirements for Ognibene?\", \"What are the material planning requirements for Hengli?\", \"What are the warranty requirements for Hengli?\", \"Can you create a table showing the warranty requirements for Ognibene and Hengli?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  \n",
    "import torch\n",
    "from transformers.agents import ReactCodeAgent\n",
    "import pandas as pd\n",
    "import HF_TOKEN\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_file_full = \"/home/vp899/projects/Agent_System/Input/Contracts/Full_Contracts_Consol.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(contract_file_full, 'r') as file_contract_full:\n",
    "    input_text_contract_full = file_contract_full.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_contract_asst = \"You are a helpful digital assistant. You will provide clear and concise answers on the input text you have been provided. You must answer in complete sentences. The input text is enclosed within <Input> and </Input>. The input text contains information on contracts with suppliers. Each individual supplier contract is enclosed within tags <Contract Between {Vendor Name} and CNH Industrial Italia SpA> and </Contract Between {Vendor Name} and CNH Industrial Italia SpA>. For example the contract information with Wipro Enterprises (P) Limited would be enclosed between the tags </Contract Between Wipro Enterprises (P) Limited and CNH Industrial Italia SpA> and </Contract Between Wipro Enterprises (P) Limited and CNH Industrial Italia SpA>. At the beginning of the contract text, there are also tags specifying the supplier name. \\n <Input> \\n\" +  input_text_contract_full + \"\\n </Input>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_chat = [{\"role\": \"system\", \"content\": system_prompt_contract_asst}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "llama31_hf_token = HF_TOKEN.HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token = llama31_hf_token)\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with columns: Experiment_Name, Prompt, Context_Length, pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer\n",
    "experiment_results_df = pd.DataFrame(columns = [\"Experiment_Name\", \"Prompt\", \"Context_Length\", \"pre_gen_max_mem_allocated_gpu0\", \"pre_gen_max_mem_allocated_gpu1\", \"pre_gen_reserved_mem_gpu0\", \"pre_gen_reserved_mem_gpu1\", \"post_gen_max_mem_allocated_gpu0\", \"post_gen_max_mem_allocated_gpu1\", \"post_gen_reserved_mem_gpu0\", \"post_gen_reserved_mem_gpu1\", \"latency\", \"llm_answer\", \"special_notes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_list = [\"flashattention_expandable\", \"torch.compile\", \"sdpa\", \"torch.compile_dynamic_mode\", \"sdpa_torch.compile\", \"8_bit\", \"8_bit_flash\"]\n",
    "experiment_list = [\"flashattention_expandable\", \"torch.compile\", \"sdpa\", \"sdpa_torch.compile\", \"8_bit\", \"8_bit_flash\"]\n",
    "experiment_list = [\"default\", \"expandable_segments\", \"flashattention_expandable\", \"8_bit\", \"8_bit_flash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_results_df[\"special_notes\"] = \"None\"\n",
    "# set the experiment name in experiment_results_df to values in the list in the same order [\"default\", \"expandable_segments\", \"flashattention_expandable\", \"8_bit\", \"8_bit_flash\"]\n",
    "# experiment_results_df[\"Experiment_Name\"] = experiment_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(experiment_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the firm period for Ognibene?',\n",
       " 'What is the firm period for Hengli?',\n",
       " 'What are the material planning requirements for Ognibene?',\n",
       " 'What are the material planning requirements for Hengli?',\n",
       " 'What are the warranty requirements for Hengli?',\n",
       " 'Can you create a table showing the warranty requirements for Ognibene and Hengli?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts_for_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment:  default\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e9316544eb54bcf80595286cf41de85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model loaded for experiment:  default\n",
      "             Running experiment:  default  with prompt:  What is the firm period for Ognibene?\n",
      "     Model deleted for experiment:  default\n",
      "Running experiment:  expandable_segments\n",
      "expandable_segments:True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad15fe3208f48678130e86fc868b5e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model loaded for experiment:  expandable_segments\n",
      "             Running experiment:  expandable_segments  with prompt:  What is the firm period for Ognibene?\n",
      "     Model deleted for experiment:  expandable_segments\n",
      "Running experiment:  flashattention_expandable\n",
      "expandable_segments:True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed7a16f8b4154f40bf3042b67ab0a198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model loaded for experiment:  flashattention_expandable\n",
      "             Running experiment:  flashattention_expandable  with prompt:  What is the firm period for Ognibene?\n",
      "     Model deleted for experiment:  flashattention_expandable\n",
      "Running experiment:  8_bit\n",
      "expandable_segments:True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b32eff4c766487e81f2d9f177db3f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model loaded for experiment:  8_bit\n",
      "             Running experiment:  8_bit  with prompt:  What is the firm period for Ognibene?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pi4_py311/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model deleted for experiment:  8_bit\n",
      "Running experiment:  8_bit_flash\n",
      "expandable_segments:True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db31e98847749b8962e0b88068bda77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model loaded for experiment:  8_bit_flash\n",
      "             Running experiment:  8_bit_flash  with prompt:  What is the firm period for Ognibene?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pi4_py311/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "# prompts_for_experiment = [\"What is the firm period for Ognibene?\"]\n",
    "# special_notes = \"Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment\"\n",
    "# special_notes = \"NOT Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment && multiple prompts\"\n",
    "# special_notes = \"Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment && single prompt\"\n",
    "special_notes = \"Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment && multiple prompts\"\n",
    "for experiment_name in experiment_list:\n",
    "    print(\"Running experiment: \", experiment_name)\n",
    "    if experiment_name != \"default\":\n",
    "        os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "        print(os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"])\n",
    "    if experiment_name == \"flashattention\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token, attn_implementation=\"flash_attention_2\",)\n",
    "    elif experiment_name == \"flashattention_expandable\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token, attn_implementation=\"flash_attention_2\",)\n",
    "    elif experiment_name == \"expandable_segments\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "    elif experiment_name == \"torch.compile\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "        model.generation_config.cache_implementation = \"static\"\n",
    "        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "    elif experiment_name == \"default\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "    elif experiment_name == \"sdpa_torch.compile\":\n",
    "        try:\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "            model.generation_config.cache_implementation = \"static\"\n",
    "            model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "        except:\n",
    "            print(\"Error in running sdpa_torch.compile\")            \n",
    "    elif experiment_name == \"torch.compile_dynamic_mode\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "        model.generation_config.cache_implementation = \"static\"\n",
    "        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", dynamic=True)\n",
    "    elif experiment_name == \"8_bit\":\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,  quantization_config=quantization_config)\n",
    "    elif experiment_name == \"8_bit_flash\":\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token, attn_implementation=\"flash_attention_2\", quantization_config=quantization_config)\n",
    "    print(\"     Model loaded for experiment: \", experiment_name)\n",
    "    for prompt in prompts_for_experiment:\n",
    "        # print experiment_name and prompt with prefix \"Running experiment: \" in a single line\n",
    "        print(\"             Running experiment: \", experiment_name, \" with prompt: \", prompt)\n",
    "\n",
    "        pre_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3) \n",
    "        pre_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "        mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "        mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "        pre_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        # pre_gen_active_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"active_bytes.all.peak\"]/1024**3)\n",
    "        pre_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        prompt_for_chat.append({\"role\": \"user\", \"content\": prompt})    \n",
    "        input_ids = tokenizer.apply_chat_template(prompt_for_chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "        start_time = time.time() \n",
    "        # if experiment name is sdpa or sdpa_torch.compile, use the sdpa_kernel context manager\n",
    "        if experiment_name == \"sdpa\" or experiment_name == \"sdpa_torch.compile\":\n",
    "            with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "                outputs = model.generate(input_ids, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,)\n",
    "        else:\n",
    "            outputs = model.generate(input_ids, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,)\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        response =  outputs[0][input_ids.shape[-1]:]\n",
    "        llm_answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        prompt_for_chat.append({\"role\": \"assistant\", \"content\": llm_answer})\n",
    "        post_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3)\n",
    "        post_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "        mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "        mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "        post_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        post_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        # add the information to the dataframe using loc\n",
    "        experiment_results_df.loc[len(experiment_results_df)] = [experiment_name, prompt, input_ids.shape[-1], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer, special_notes]\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(\"     Model deleted for experiment: \", experiment_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "# experiment_results_df.to_csv(\"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/experiment_results_expandable.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment_Name</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Context_Length</th>\n",
       "      <th>pre_gen_max_mem_allocated_gpu0</th>\n",
       "      <th>pre_gen_max_mem_allocated_gpu1</th>\n",
       "      <th>pre_gen_reserved_mem_gpu0</th>\n",
       "      <th>pre_gen_reserved_mem_gpu1</th>\n",
       "      <th>post_gen_max_mem_allocated_gpu0</th>\n",
       "      <th>post_gen_max_mem_allocated_gpu1</th>\n",
       "      <th>post_gen_reserved_mem_gpu0</th>\n",
       "      <th>post_gen_reserved_mem_gpu1</th>\n",
       "      <th>latency</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>special_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>What is the firm period for Ognibene?</td>\n",
       "      <td>70483</td>\n",
       "      <td>6.67</td>\n",
       "      <td>8.29</td>\n",
       "      <td>6.72</td>\n",
       "      <td>8.36</td>\n",
       "      <td>18.28</td>\n",
       "      <td>20.44</td>\n",
       "      <td>22.37</td>\n",
       "      <td>23.88</td>\n",
       "      <td>19.561571</td>\n",
       "      <td>According to the Master Supply Agreement betwe...</td>\n",
       "      <td>NOT Set pytorch_cuda_alloc_conf to expandable_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>expandable_segments</td>\n",
       "      <td>What is the firm period for Ognibene?</td>\n",
       "      <td>70546</td>\n",
       "      <td>6.68</td>\n",
       "      <td>20.44</td>\n",
       "      <td>6.74</td>\n",
       "      <td>23.88</td>\n",
       "      <td>18.29</td>\n",
       "      <td>20.45</td>\n",
       "      <td>22.14</td>\n",
       "      <td>23.90</td>\n",
       "      <td>18.410501</td>\n",
       "      <td>According to the Master Supply Agreement betwe...</td>\n",
       "      <td>NOT Set pytorch_cuda_alloc_conf to expandable_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flashattention_expandable</td>\n",
       "      <td>What is the firm period for Ognibene?</td>\n",
       "      <td>70605</td>\n",
       "      <td>6.68</td>\n",
       "      <td>20.45</td>\n",
       "      <td>6.74</td>\n",
       "      <td>23.90</td>\n",
       "      <td>18.30</td>\n",
       "      <td>20.46</td>\n",
       "      <td>22.96</td>\n",
       "      <td>24.71</td>\n",
       "      <td>14.084867</td>\n",
       "      <td>According to the Master Supply Agreement betwe...</td>\n",
       "      <td>NOT Set pytorch_cuda_alloc_conf to expandable_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8_bit</td>\n",
       "      <td>What is the firm period for Ognibene?</td>\n",
       "      <td>70664</td>\n",
       "      <td>3.35</td>\n",
       "      <td>20.46</td>\n",
       "      <td>3.41</td>\n",
       "      <td>24.71</td>\n",
       "      <td>18.69</td>\n",
       "      <td>23.02</td>\n",
       "      <td>21.67</td>\n",
       "      <td>25.74</td>\n",
       "      <td>25.554915</td>\n",
       "      <td>According to the Master Supply Agreement betwe...</td>\n",
       "      <td>NOT Set pytorch_cuda_alloc_conf to expandable_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8_bit_flash</td>\n",
       "      <td>What is the firm period for Ognibene?</td>\n",
       "      <td>70723</td>\n",
       "      <td>18.69</td>\n",
       "      <td>5.42</td>\n",
       "      <td>21.67</td>\n",
       "      <td>5.46</td>\n",
       "      <td>18.71</td>\n",
       "      <td>23.03</td>\n",
       "      <td>21.67</td>\n",
       "      <td>24.66</td>\n",
       "      <td>25.205945</td>\n",
       "      <td>According to the Master Supply Agreement betwe...</td>\n",
       "      <td>NOT Set pytorch_cuda_alloc_conf to expandable_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Experiment_Name                                 Prompt  \\\n",
       "0                    default  What is the firm period for Ognibene?   \n",
       "1        expandable_segments  What is the firm period for Ognibene?   \n",
       "2  flashattention_expandable  What is the firm period for Ognibene?   \n",
       "3                      8_bit  What is the firm period for Ognibene?   \n",
       "4                8_bit_flash  What is the firm period for Ognibene?   \n",
       "\n",
       "   Context_Length pre_gen_max_mem_allocated_gpu0  \\\n",
       "0           70483                           6.67   \n",
       "1           70546                           6.68   \n",
       "2           70605                           6.68   \n",
       "3           70664                           3.35   \n",
       "4           70723                          18.69   \n",
       "\n",
       "  pre_gen_max_mem_allocated_gpu1 pre_gen_reserved_mem_gpu0  \\\n",
       "0                           8.29                      6.72   \n",
       "1                          20.44                      6.74   \n",
       "2                          20.45                      6.74   \n",
       "3                          20.46                      3.41   \n",
       "4                           5.42                     21.67   \n",
       "\n",
       "  pre_gen_reserved_mem_gpu1 post_gen_max_mem_allocated_gpu0  \\\n",
       "0                      8.36                           18.28   \n",
       "1                     23.88                           18.29   \n",
       "2                     23.90                           18.30   \n",
       "3                     24.71                           18.69   \n",
       "4                      5.46                           18.71   \n",
       "\n",
       "  post_gen_max_mem_allocated_gpu1 post_gen_reserved_mem_gpu0  \\\n",
       "0                           20.44                      22.37   \n",
       "1                           20.45                      22.14   \n",
       "2                           20.46                      22.96   \n",
       "3                           23.02                      21.67   \n",
       "4                           23.03                      21.67   \n",
       "\n",
       "  post_gen_reserved_mem_gpu1    latency  \\\n",
       "0                      23.88  19.561571   \n",
       "1                      23.90  18.410501   \n",
       "2                      24.71  14.084867   \n",
       "3                      25.74  25.554915   \n",
       "4                      24.66  25.205945   \n",
       "\n",
       "                                          llm_answer  \\\n",
       "0  According to the Master Supply Agreement betwe...   \n",
       "1  According to the Master Supply Agreement betwe...   \n",
       "2  According to the Master Supply Agreement betwe...   \n",
       "3  According to the Master Supply Agreement betwe...   \n",
       "4  According to the Master Supply Agreement betwe...   \n",
       "\n",
       "                                       special_notes  \n",
       "0  NOT Set pytorch_cuda_alloc_conf to expandable_...  \n",
       "1  NOT Set pytorch_cuda_alloc_conf to expandable_...  \n",
       "2  NOT Set pytorch_cuda_alloc_conf to expandable_...  \n",
       "3  NOT Set pytorch_cuda_alloc_conf to expandable_...  \n",
       "4  NOT Set pytorch_cuda_alloc_conf to expandable_...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment_Name</th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Context_Length</th>\n",
       "      <th>pre_gen_max_mem_allocated_gpu0</th>\n",
       "      <th>pre_gen_max_mem_allocated_gpu1</th>\n",
       "      <th>pre_gen_reserved_mem_gpu0</th>\n",
       "      <th>pre_gen_reserved_mem_gpu1</th>\n",
       "      <th>post_gen_max_mem_allocated_gpu0</th>\n",
       "      <th>post_gen_max_mem_allocated_gpu1</th>\n",
       "      <th>post_gen_reserved_mem_gpu0</th>\n",
       "      <th>post_gen_reserved_mem_gpu1</th>\n",
       "      <th>latency</th>\n",
       "      <th>llm_answer</th>\n",
       "      <th>special_notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>default</td>\n",
       "      <td>What is the firm period for Ognibene?</td>\n",
       "      <td>70806</td>\n",
       "      <td>18.71</td>\n",
       "      <td>8.30</td>\n",
       "      <td>21.67</td>\n",
       "      <td>8.38</td>\n",
       "      <td>18.71</td>\n",
       "      <td>20.49</td>\n",
       "      <td>22.20</td>\n",
       "      <td>23.96</td>\n",
       "      <td>18.530892</td>\n",
       "      <td>According to the Master Supply Agreement betwe...</td>\n",
       "      <td>NOT Set pytorch_cuda_alloc_conf to expandable_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>expandable_segments</td>\n",
       "      <td>What is the firm period for Ognibene?</td>\n",
       "      <td>70865</td>\n",
       "      <td>18.71</td>\n",
       "      <td>8.30</td>\n",
       "      <td>22.20</td>\n",
       "      <td>8.38</td>\n",
       "      <td>18.71</td>\n",
       "      <td>20.50</td>\n",
       "      <td>22.20</td>\n",
       "      <td>23.96</td>\n",
       "      <td>21.858373</td>\n",
       "      <td>According to the Master Supply Agreement betwe...</td>\n",
       "      <td>NOT Set pytorch_cuda_alloc_conf to expandable_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flashattention_expandable</td>\n",
       "      <td>What is the firm period for Ognibene?</td>\n",
       "      <td>70946</td>\n",
       "      <td>18.71</td>\n",
       "      <td>8.30</td>\n",
       "      <td>22.20</td>\n",
       "      <td>8.38</td>\n",
       "      <td>18.71</td>\n",
       "      <td>20.51</td>\n",
       "      <td>22.20</td>\n",
       "      <td>22.90</td>\n",
       "      <td>16.705445</td>\n",
       "      <td>According to the Master Supply Agreement betwe...</td>\n",
       "      <td>NOT Set pytorch_cuda_alloc_conf to expandable_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8_bit</td>\n",
       "      <td>What is the firm period for Ognibene?</td>\n",
       "      <td>71051</td>\n",
       "      <td>18.71</td>\n",
       "      <td>5.42</td>\n",
       "      <td>22.20</td>\n",
       "      <td>5.46</td>\n",
       "      <td>18.78</td>\n",
       "      <td>23.11</td>\n",
       "      <td>22.20</td>\n",
       "      <td>23.94</td>\n",
       "      <td>42.264166</td>\n",
       "      <td>According to the Master Supply Agreement betwe...</td>\n",
       "      <td>NOT Set pytorch_cuda_alloc_conf to expandable_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8_bit_flash</td>\n",
       "      <td>What is the firm period for Ognibene?</td>\n",
       "      <td>71172</td>\n",
       "      <td>18.78</td>\n",
       "      <td>5.42</td>\n",
       "      <td>22.20</td>\n",
       "      <td>5.46</td>\n",
       "      <td>18.80</td>\n",
       "      <td>23.14</td>\n",
       "      <td>22.20</td>\n",
       "      <td>24.80</td>\n",
       "      <td>31.015537</td>\n",
       "      <td>According to the Master Supply Agreement betwe...</td>\n",
       "      <td>NOT Set pytorch_cuda_alloc_conf to expandable_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Experiment_Name                                 Prompt  \\\n",
       "5                    default  What is the firm period for Ognibene?   \n",
       "6        expandable_segments  What is the firm period for Ognibene?   \n",
       "7  flashattention_expandable  What is the firm period for Ognibene?   \n",
       "8                      8_bit  What is the firm period for Ognibene?   \n",
       "9                8_bit_flash  What is the firm period for Ognibene?   \n",
       "\n",
       "   Context_Length pre_gen_max_mem_allocated_gpu0  \\\n",
       "5           70806                          18.71   \n",
       "6           70865                          18.71   \n",
       "7           70946                          18.71   \n",
       "8           71051                          18.71   \n",
       "9           71172                          18.78   \n",
       "\n",
       "  pre_gen_max_mem_allocated_gpu1 pre_gen_reserved_mem_gpu0  \\\n",
       "5                           8.30                     21.67   \n",
       "6                           8.30                     22.20   \n",
       "7                           8.30                     22.20   \n",
       "8                           5.42                     22.20   \n",
       "9                           5.42                     22.20   \n",
       "\n",
       "  pre_gen_reserved_mem_gpu1 post_gen_max_mem_allocated_gpu0  \\\n",
       "5                      8.38                           18.71   \n",
       "6                      8.38                           18.71   \n",
       "7                      8.38                           18.71   \n",
       "8                      5.46                           18.78   \n",
       "9                      5.46                           18.80   \n",
       "\n",
       "  post_gen_max_mem_allocated_gpu1 post_gen_reserved_mem_gpu0  \\\n",
       "5                           20.49                      22.20   \n",
       "6                           20.50                      22.20   \n",
       "7                           20.51                      22.20   \n",
       "8                           23.11                      22.20   \n",
       "9                           23.14                      22.20   \n",
       "\n",
       "  post_gen_reserved_mem_gpu1    latency  \\\n",
       "5                      23.96  18.530892   \n",
       "6                      23.96  21.858373   \n",
       "7                      22.90  16.705445   \n",
       "8                      23.94  42.264166   \n",
       "9                      24.80  31.015537   \n",
       "\n",
       "                                          llm_answer  \\\n",
       "5  According to the Master Supply Agreement betwe...   \n",
       "6  According to the Master Supply Agreement betwe...   \n",
       "7  According to the Master Supply Agreement betwe...   \n",
       "8  According to the Master Supply Agreement betwe...   \n",
       "9  According to the Master Supply Agreement betwe...   \n",
       "\n",
       "                                       special_notes  \n",
       "5  NOT Set pytorch_cuda_alloc_conf to expandable_...  \n",
       "6  NOT Set pytorch_cuda_alloc_conf to expandable_...  \n",
       "7  NOT Set pytorch_cuda_alloc_conf to expandable_...  \n",
       "8  NOT Set pytorch_cuda_alloc_conf to expandable_...  \n",
       "9  NOT Set pytorch_cuda_alloc_conf to expandable_...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print rows 5-9 of the dataframe\n",
    "experiment_results_df[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete rows 5 to 9 from the dataframe\n",
    "experiment_results_df.drop(index=range(5,10), inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi4_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
