{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of experiments to run\n",
    "## 1] Default scenario\n",
    "## 2] flashattention\n",
    "## 3] expandable_segments: Set expandable segments -- check if it works in notebook\n",
    "## 4] flash_expandable: flashattention + expandable segments\n",
    "## 5] torch.compile\n",
    "## 6] sdpa\n",
    "## 7] sdpa_torch.compile: sdpa + torch.compile\n",
    "## 8] 8_bit: 8-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_for_experiment = [\"What is the firm period for Ognibene?\", \"What is the firm period for Hengli?\", \"What are the material planning requirements for Ognibene?\", \"What are the material planning requirements for Hengli?\", \"What are the warranty requirements for Hengli?\", \"Can you create a table showing the warranty requirements for Ognibene and Hengli?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  \n",
    "import torch\n",
    "from transformers.agents import ReactCodeAgent\n",
    "import pandas as pd\n",
    "import HF_TOKEN\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_file_full = \"/home/vp899/projects/Agent_System/Input/Contracts/Full_Contracts_Consol.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(contract_file_full, 'r') as file_contract_full:\n",
    "    input_text_contract_full = file_contract_full.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_contract_asst = \"You are a helpful digital assistant. You will provide clear and concise answers on the input text you have been provided. You must answer in complete sentences. The input text is enclosed within <Input> and </Input>. The input text contains information on contracts with suppliers. Each individual supplier contract is enclosed within tags <Contract Between {Vendor Name} and CNH Industrial Italia SpA> and </Contract Between {Vendor Name} and CNH Industrial Italia SpA>. For example the contract information with Wipro Enterprises (P) Limited would be enclosed between the tags </Contract Between Wipro Enterprises (P) Limited and CNH Industrial Italia SpA> and </Contract Between Wipro Enterprises (P) Limited and CNH Industrial Italia SpA>. At the beginning of the contract text, there are also tags specifying the supplier name. \\n <Input> \\n\" +  input_text_contract_full + \"\\n </Input>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_chat = [{\"role\": \"system\", \"content\": system_prompt_contract_asst}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "llama31_hf_token = HF_TOKEN.HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token = llama31_hf_token)\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with columns: Experiment_Name, Prompt, Context_Length, pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer\n",
    "# experiment_results_df = pd.DataFrame(columns = [\"Experiment_Name\",\"PYTORCH_CUDA_ALLOC_CONF\", \"Prompt\", \"Context_Length\", \"pre_gen_max_mem_allocated_gpu0\", \"pre_gen_max_mem_allocated_gpu1\", \"pre_gen_reserved_mem_gpu0\", \"pre_gen_reserved_mem_gpu1\", \"post_gen_max_mem_allocated_gpu0\", \"post_gen_max_mem_allocated_gpu1\", \"post_gen_reserved_mem_gpu0\", \"post_gen_reserved_mem_gpu1\", \"latency\", \"llm_answer\", \"status\", \"exception\",\"experiment_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_name = \"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output\"\n",
    "# save the dataframe to a csv file in the output folder\n",
    "# experiment_results_df.to_csv(output_folder_name + \"/batch_experiment_results.csv\", index = False)\n",
    "# experiment_results_df = pd.read_csv(output_folder_name + \"/batch_experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_list = [\"default\", \"flashattention\", \"8_bit\", \"8_bit_flash\"]\n",
    "experiment_list = [\"default\"]\n",
    "experiment_list = [\"flashattention\", \"8_bit\", \"8_bit_flash\"]\n",
    "PYTORCH_CUDA_ALLOC_CONF_value = \"expandable_segments:not set\"\n",
    "# PYTORCH_CUDA_ALLOC_CONF_value = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/batch_run_log.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment:  flashattention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1994ce879a0a4eba97b868d5fc5c49c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model loaded for experiment:  flashattention\n",
      "     Finishing loading model for experiment:  flashattention\n",
      "             Running experiment:  flashattention  with prompt:  What is the firm period for Ognibene?\n",
      "             In TRY section\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Running experiment:  flashattention  with prompt:  What is the firm period for Hengli?\n",
      "             In TRY section\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot set a row with mismatched columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m             In TRY section\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, eos_token_id\u001b[38;5;241m=\u001b[39mterminators, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m, top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,)\n\u001b[1;32m     67\u001b[0m response \u001b[38;5;241m=\u001b[39m  outputs[\u001b[38;5;241m0\u001b[39m][input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n",
      "File \u001b[0;32m/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/pi2_py311/lib/python3.11/site-packages/transformers/generation/utils.py:1989\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1989\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   1990\u001b[0m         input_ids,\n\u001b[1;32m   1991\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   1992\u001b[0m         logits_warper\u001b[38;5;241m=\u001b[39mprepared_logits_warper,\n\u001b[1;32m   1993\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   1994\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   1995\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   1996\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   1997\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1998\u001b[0m     )\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/pi2_py311/lib/python3.11/site-packages/transformers/generation/utils.py:2932\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2931\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2932\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n",
      "File \u001b[0;32m/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/pi2_py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/anaconda/envs/pi2_py311/lib/python3.11/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/anaconda/envs/pi2_py311/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1161\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1160\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n\u001b[0;32m-> 1161\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m   1163\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 33.68 GiB. GPU \u0001 has a total capacity of 79.15 GiB of which 21.34 GiB is free. Process 1167465 has 6.54 GiB memory in use. Including non-PyTorch memory, this process has 51.17 GiB memory in use. Of the allocated memory 30.52 GiB is allocated by PyTorch, and 20.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     86\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 87\u001b[0m experiment_results_df\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(experiment_results_df)] \u001b[38;5;241m=\u001b[39m [experiment_name, PYTORCH_CUDA_ALLOC_CONF_value, prompt, input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, status, date_time]\n\u001b[1;32m     88\u001b[0m experiment_results_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/batch_experiment_results.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(log_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_log:\n",
      "File \u001b[0;32m/anaconda/envs/pi2_py311/lib/python3.11/site-packages/pandas/core/indexing.py:885\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    884\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 885\u001b[0m iloc\u001b[38;5;241m.\u001b[39m_setitem_with_indexer(indexer, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/anaconda/envs/pi2_py311/lib/python3.11/site-packages/pandas/core/indexing.py:1883\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1880\u001b[0m     indexer, missing \u001b[38;5;241m=\u001b[39m convert_missing_indexer(indexer)\n\u001b[1;32m   1882\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m-> 1883\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_missing(indexer, value)\n\u001b[1;32m   1884\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1887\u001b[0m     \u001b[38;5;66;03m# must come after setting of missing\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/pi2_py311/lib/python3.11/site-packages/pandas/core/indexing.py:2219\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_missing\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m   2216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_list_like_indexer(value):\n\u001b[1;32m   2217\u001b[0m         \u001b[38;5;66;03m# must have conforming columns\u001b[39;00m\n\u001b[1;32m   2218\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns):\n\u001b[0;32m-> 2219\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot set a row with mismatched columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2221\u001b[0m     value \u001b[38;5;241m=\u001b[39m Series(value, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39mcolumns, name\u001b[38;5;241m=\u001b[39mindexer)\n\u001b[1;32m   2223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj):\n\u001b[1;32m   2224\u001b[0m     \u001b[38;5;66;03m# We will ignore the existing dtypes instead of using\u001b[39;00m\n\u001b[1;32m   2225\u001b[0m     \u001b[38;5;66;03m#  internals.concat logic\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot set a row with mismatched columns"
     ]
    }
   ],
   "source": [
    "# prompts_for_experiment = [\"What is the firm period for Ognibene?\"]\n",
    "# special_notes = \"Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment\"\n",
    "# special_notes = \"NOT Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment && multiple prompts\"\n",
    "# special_notes = \"Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment && single prompt\"\n",
    "special_notes = \"Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment && multiple prompts\"\n",
    "date_time = time.strftime(\"%d-%b-%Y %H:%M:%S\")\n",
    "with open(log_file, 'a') as file_log:\n",
    "    file_log.write(f\"**** STARTING EXPERIMENTS WITH {PYTORCH_CUDA_ALLOC_CONF_value} at {date_time}****\\n\")\n",
    "for experiment_name in experiment_list:\n",
    "    print(\"Running experiment: \", experiment_name)\n",
    "    # write the experiment name to the log file\n",
    "    with open(log_file, 'a') as file_log:\n",
    "        file_log.write(f\"   Running experiment: {experiment_name}\\n\") # 1 tab\n",
    "    if experiment_name == \"flashattention\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token, attn_implementation=\"flash_attention_2\",)\n",
    "        print(\"     Model loaded for experiment: \", experiment_name)\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"       Model loaded for experiment: {experiment_name}\\n\") #2 tabs\n",
    "    elif experiment_name == \"default\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "        print(\"     Model loaded for experiment: \", experiment_name)\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"       Model loaded for experiment: {experiment_name}\\n\") #2 tabs\n",
    "    elif experiment_name == \"8_bit\":\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,  quantization_config=quantization_config)\n",
    "        print(\"     Model loaded for experiment: \", experiment_name)\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"       Model loaded for experiment: {experiment_name}\\n\") #2 tabs\n",
    "    elif experiment_name == \"8_bit_flash\":\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token, attn_implementation=\"flash_attention_2\", quantization_config=quantization_config)\n",
    "        print(\"     Model loaded for experiment: \", experiment_name)\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"       Model loaded for experiment: {experiment_name}\\n\") #2 tabs\n",
    "    print(\"     Finishing loading model for experiment: \", experiment_name)\n",
    "    with open(log_file, 'a') as file_log:\n",
    "        file_log.write(f\"       Finishing loading model for experiment: {experiment_name}\\n\") #2 tabs\n",
    "    for prompt in prompts_for_experiment:\n",
    "        # print experiment_name and prompt with prefix \"Running experiment: \" in a single line\n",
    "        print(\"             Running experiment: \", experiment_name, \" with prompt: \", prompt)\n",
    "        experiment_results_df = pd.read_csv(output_folder_name + \"/batch_experiment_results.csv\")\n",
    "        # write the experiment name and prompt to the log file\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"           Running prompt: {prompt}\\n\") #3 tabs\n",
    "\n",
    "        pre_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3) \n",
    "        pre_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "        mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "        mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "        pre_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        # pre_gen_active_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"active_bytes.all.peak\"]/1024**3)\n",
    "        pre_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        prompt_for_chat.append({\"role\": \"user\", \"content\": prompt})    \n",
    "        input_ids = tokenizer.apply_chat_template(prompt_for_chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "        start_time = time.time() \n",
    "        # if experiment name is sdpa or sdpa_torch.compile, use the sdpa_kernel context manager\n",
    "        # write pre-gen memory stats to the log file\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"               Pre-gen memory stats - allocated 0,1 & reserved 0,1: {pre_gen_max_mem_allocated_gpu0}, {pre_gen_max_mem_allocated_gpu1}, {pre_gen_reserved_mem_gpu0}, {pre_gen_reserved_mem_gpu1}\\n\") # 4 tabs\n",
    "        try:\n",
    "            exception_name = \"\"\n",
    "            with open(log_file, 'a') as file_log:\n",
    "                file_log.write(f\"               In TRY section\\n\") # 4 tabs\n",
    "            print(\"             In TRY section\")\n",
    "            outputs = model.generate(input_ids, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,)\n",
    "            response =  outputs[0][input_ids.shape[-1]:]\n",
    "            llm_answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "            end_time = time.time()\n",
    "            post_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3)\n",
    "            post_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "            mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "            mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "            post_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "            post_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "            latency = end_time - start_time\n",
    "            status = \"Success\"\n",
    "            experiment_results_df.loc[len(experiment_results_df)] = [experiment_name, PYTORCH_CUDA_ALLOC_CONF_value, prompt, input_ids.shape[-1], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer, status, exception_name , date_time]\n",
    "            experiment_results_df.to_csv(\"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/batch_experiment_results.csv\", index=False)\n",
    "            with open(log_file, 'a') as file_log:\n",
    "                file_log.write(f\"               Finished prompt with status: {status}\\n\") # 4 tabs\n",
    "           \n",
    "        except Exception as exception_name:\n",
    "            llm_answer = \"\"\n",
    "            end_time = time.time()\n",
    "            status = \"Failed\"\n",
    "            experiment_results_df.loc[len(experiment_results_df)] = [experiment_name, PYTORCH_CUDA_ALLOC_CONF_value, prompt, input_ids.shape[-1], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, 0, 0, 0, 0, 0, \"\", status, date_time]\n",
    "            experiment_results_df.to_csv(\"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/batch_experiment_results.csv\", index=False)\n",
    "            with open(log_file, 'a') as file_log:\n",
    "                file_log.write(f\"               Finished prompt with status: {status} & exception: {exception_name}\\n\") # 3 tabs\n",
    "            print(\"             Failed with exception: \", exception_name) # 4 tabs\n",
    "            #del model\n",
    "            #gc.collect()\n",
    "            #torch.cuda.empty_cache()\n",
    "            #torch.cuda.reset_peak_memory_stats()\n",
    "            break\n",
    "            \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(\"     Model deleted for experiment: \", experiment_name)\n",
    "    with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"       Model deleted for experiment: {experiment_name}\\n\") #2 tabs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "# experiment_results_df.to_csv(\"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/batch_experiment_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print rows 5-9 of the dataframe\n",
    "experiment_results_df[5:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi2_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
