{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of experiments to run\n",
    "## 1] Default scenario\n",
    "## 2] flashattention\n",
    "## 3] expandable_segments: Set expandable segments -- check if it works in notebook\n",
    "## 4] flash_expandable: flashattention + expandable segments\n",
    "## 5] torch.compile\n",
    "## 6] sdpa\n",
    "## 7] sdpa_torch.compile: sdpa + torch.compile\n",
    "## 8] 8_bit: 8-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_for_experiment = [\"What is the firm period for Ognibene?\", \"What is the firm period for Hengli?\", \"What are the material planning requirements for Ognibene?\", \"What are the material planning requirements for Hengli?\", \"What are the warranty requirements for Hengli?\", \"Can you create a table showing the warranty requirements for Ognibene and Hengli?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  \n",
    "import torch\n",
    "from transformers.agents import ReactCodeAgent\n",
    "import pandas as pd\n",
    "import HF_TOKEN\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_file_full = \"/home/vp899/projects/Agent_System/Input/Contracts/Full_Contracts_Consol.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(contract_file_full, 'r') as file_contract_full:\n",
    "    input_text_contract_full = file_contract_full.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_contract_asst = \"You are a helpful digital assistant. You will provide clear and concise answers on the input text you have been provided. You must answer in complete sentences. The input text is enclosed within <Input> and </Input>. The input text contains information on contracts with suppliers. Each individual supplier contract is enclosed within tags <Contract Between {Vendor Name} and CNH Industrial Italia SpA> and </Contract Between {Vendor Name} and CNH Industrial Italia SpA>. For example the contract information with Wipro Enterprises (P) Limited would be enclosed between the tags </Contract Between Wipro Enterprises (P) Limited and CNH Industrial Italia SpA> and </Contract Between Wipro Enterprises (P) Limited and CNH Industrial Italia SpA>. At the beginning of the contract text, there are also tags specifying the supplier name. \\n <Input> \\n\" +  input_text_contract_full + \"\\n </Input>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_chat = [{\"role\": \"system\", \"content\": system_prompt_contract_asst}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "llama31_hf_token = HF_TOKEN.HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token = llama31_hf_token)\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with columns: Experiment_Name, Prompt, Context_Length, pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer\n",
    "# experiment_results_df = pd.DataFrame(columns = [\"Experiment_Name\",\"PYTORCH_CUDA_ALLOC_CONF\", \"Prompt\", \"Context_Length\", \"pre_gen_max_mem_allocated_gpu0\", \"pre_gen_max_mem_allocated_gpu1\", \"pre_gen_reserved_mem_gpu0\", \"pre_gen_reserved_mem_gpu1\", \"post_gen_max_mem_allocated_gpu0\", \"post_gen_max_mem_allocated_gpu1\", \"post_gen_reserved_mem_gpu0\", \"post_gen_reserved_mem_gpu1\", \"latency\", \"llm_answer\", \"status\", \"exception\",\"experiment_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_name = \"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output\"\n",
    "# save the dataframe to a csv file in the output folder\n",
    "# experiment_results_df.to_csv(output_folder_name + \"/batch_experiment_results.csv\", index = False)\n",
    "# experiment_results_df = pd.read_csv(output_folder_name + \"/batch_experiment_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_list = [\"default\", \"flashattention\", \"8_bit\", \"8_bit_flash\"]\n",
    "# experiment_list = [\"default\"]\n",
    "experiment_list = [\"flashattention\", \"8_bit\", \"8_bit_flash\"]\n",
    "# PYTORCH_CUDA_ALLOC_CONF_value = \"expandable_segments:not set\" # USED for first round of experiments\n",
    "PYTORCH_CUDA_ALLOC_CONF_value = \"expandable_segments:True -- in terminal\" # USED for second round of experiments\n",
    "# PYTORCH_CUDA_ALLOC_CONF_value = \"pytorch2.5.1 & transformers4.46.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = \"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/batch_run_log.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment:  flashattention\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c4cd13d2f34226b53cc12c42073716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model loaded for experiment:  flashattention\n",
      "     Finishing loading model for experiment:  flashattention\n",
      "             Running experiment:  flashattention  with prompt  1 :  What is the firm period for Ognibene?\n",
      "             In TRY section\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Running experiment:  flashattention  with prompt  2 :  What is the firm period for Hengli?\n",
      "             In TRY section\n",
      "             Failed with exception:  CUDA out of memory. Tried to allocate 33.68 GiB. GPU \u0001 has a total capacity of 79.15 GiB of which 21.34 GiB is free. Process 1167465 has 6.54 GiB memory in use. Including non-PyTorch memory, this process has 51.17 GiB memory in use. Of the allocated memory 30.52 GiB is allocated by PyTorch, and 20.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "     Model deleted for experiment:  flashattention\n",
      "Running experiment:  8_bit\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3602f314c69d499c9f2962bdb54ce52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model loaded for experiment:  8_bit\n",
      "     Finishing loading model for experiment:  8_bit\n",
      "             Running experiment:  8_bit  with prompt  1 :  What is the firm period for Ognibene?\n",
      "             In TRY section\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pi2_py311/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Failed with exception:  CUDA out of memory. Tried to allocate 33.69 GiB. GPU \u0001 has a total capacity of 79.15 GiB of which 21.83 GiB is free. Process 1167465 has 6.54 GiB memory in use. Including non-PyTorch memory, this process has 50.68 GiB memory in use. Of the allocated memory 36.74 GiB is allocated by PyTorch, and 13.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "     Model deleted for experiment:  8_bit\n",
      "Running experiment:  8_bit_flash\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a375a82b9a4478bcb19b5fbaaff3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Model loaded for experiment:  8_bit_flash\n",
      "     Finishing loading model for experiment:  8_bit_flash\n",
      "             Running experiment:  8_bit_flash  with prompt  1 :  What is the firm period for Ognibene?\n",
      "             In TRY section\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/pi2_py311/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Failed with exception:  CUDA out of memory. Tried to allocate 33.70 GiB. GPU \u0001 has a total capacity of 79.15 GiB of which 30.34 GiB is free. Process 1167465 has 6.54 GiB memory in use. Including non-PyTorch memory, this process has 42.16 GiB memory in use. Of the allocated memory 33.87 GiB is allocated by PyTorch, and 7.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "     Model deleted for experiment:  8_bit_flash\n"
     ]
    }
   ],
   "source": [
    "# prompts_for_experiment = [\"What is the firm period for Ognibene?\"]\n",
    "# special_notes = \"Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment\"\n",
    "# special_notes = \"NOT Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment && multiple prompts\"\n",
    "# special_notes = \"Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment && single prompt\"\n",
    "special_notes = \"Set pytorch_cuda_alloc_conf to expandable_segments:True in terminal before running the experiment && multiple prompts\"\n",
    "date_time = time.strftime(\"%d-%b-%Y %H:%M:%S\")\n",
    "with open(log_file, 'a') as file_log:\n",
    "    # file_log.write(f\"**** STARTING EXPERIMENTS WITH {PYTORCH_CUDA_ALLOC_CONF_value} at {date_time}****\\n\")\n",
    "    file_log.write(f\"**** STARTING EXPERIMENTS WITH pytorch2.5.1 & transformers4.46.1 at {date_time}****\\n\")\n",
    "for experiment_name in experiment_list:\n",
    "    print(\"Running experiment: \", experiment_name)\n",
    "    # write the experiment name to the log file\n",
    "    with open(log_file, 'a') as file_log:\n",
    "        file_log.write(f\"   Running experiment: {experiment_name}\\n\") # 1 tab\n",
    "    if experiment_name == \"flashattention\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token, attn_implementation=\"flash_attention_2\",)\n",
    "        print(\"     Model loaded for experiment: \", experiment_name)\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"       Model loaded for experiment: {experiment_name}\\n\") #2 tabs\n",
    "    elif experiment_name == \"default\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "        print(\"     Model loaded for experiment: \", experiment_name)\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"       Model loaded for experiment: {experiment_name}\\n\") #2 tabs\n",
    "    elif experiment_name == \"8_bit\":\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,  quantization_config=quantization_config)\n",
    "        print(\"     Model loaded for experiment: \", experiment_name)\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"       Model loaded for experiment: {experiment_name}\\n\") #2 tabs\n",
    "    elif experiment_name == \"8_bit_flash\":\n",
    "        quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token, attn_implementation=\"flash_attention_2\", quantization_config=quantization_config)\n",
    "        print(\"     Model loaded for experiment: \", experiment_name)\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"       Model loaded for experiment: {experiment_name}\\n\") #2 tabs\n",
    "    print(\"     Finishing loading model for experiment: \", experiment_name)\n",
    "    with open(log_file, 'a') as file_log:\n",
    "        file_log.write(f\"       Finishing loading model for experiment: {experiment_name}\\n\") #2 tabs\n",
    "    prompt_count = 1\n",
    "    for prompt in prompts_for_experiment:\n",
    "        # print experiment_name and prompt with prefix \"Running experiment: \" in a single line\n",
    "        print(\"             Running experiment: \", experiment_name, \" with prompt \", prompt_count,\": \", prompt)\n",
    "        experiment_results_df = pd.read_csv(output_folder_name + \"/batch_experiment_results.csv\")\n",
    "        # write the experiment name and prompt to the log file\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"           Running prompt: {prompt}\\n\") #3 tabs\n",
    "\n",
    "        pre_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3) \n",
    "        pre_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "        mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "        mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "        pre_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        # pre_gen_active_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"active_bytes.all.peak\"]/1024**3)\n",
    "        pre_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        prompt_for_chat.append({\"role\": \"user\", \"content\": prompt})    \n",
    "        input_ids = tokenizer.apply_chat_template(prompt_for_chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "        start_time = time.time() \n",
    "        # if experiment name is sdpa or sdpa_torch.compile, use the sdpa_kernel context manager\n",
    "        # write pre-gen memory stats to the log file\n",
    "        with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"               Pre-gen memory stats - allocated GPU0,GPU1 & reserved GPU0,GPU1: {pre_gen_max_mem_allocated_gpu0}, {pre_gen_max_mem_allocated_gpu1} & {pre_gen_reserved_mem_gpu0}, {pre_gen_reserved_mem_gpu1}\\n\") # 4 tabs\n",
    "        try:\n",
    "            exception_name = \"None\"\n",
    "            print(\"             In TRY section\")\n",
    "            with open(log_file, 'a') as file_log:\n",
    "                file_log.write(f\"               Starting generation of response\\n\") # 4 tabs\n",
    "            outputs = model.generate(input_ids, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,)\n",
    "            response =  outputs[0][input_ids.shape[-1]:]\n",
    "            llm_answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "            end_time = time.time()\n",
    "            post_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3)\n",
    "            post_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "            mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "            mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "            post_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "            post_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "            latency = end_time - start_time\n",
    "            status = \"Success\"\n",
    "            experiment_results_df.loc[len(experiment_results_df)] = [experiment_name, PYTORCH_CUDA_ALLOC_CONF_value, prompt, input_ids.shape[-1], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer, status, exception_name , date_time]\n",
    "            experiment_results_df.to_csv(\"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/batch_experiment_results.csv\", index=False)\n",
    "            with open(log_file, 'a') as file_log:\n",
    "                file_log.write(f\"               Finished generating response with status: {status}\\n\") # 4 tabs\n",
    "                # write post-gen memory stats to the log file\n",
    "                file_log.write(f\"               Post-gen memory stats - allocated GPU0,GPU1 & reserved GPU0,GPU1: {post_gen_max_mem_allocated_gpu0}, {post_gen_max_mem_allocated_gpu1} & {post_gen_reserved_mem_gpu0}, {post_gen_reserved_mem_gpu1}\\n\") # 4 tabs\n",
    "           \n",
    "        except Exception as exception_name:\n",
    "            status = \"Failed\"\n",
    "            print(\"             Failed with exception: \", exception_name) # 4 tabs\n",
    "            \n",
    "            llm_answer = \"\"\n",
    "            end_time = time.time()\n",
    "            # calculate the post-gen memory stats\n",
    "            post_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3)\n",
    "            post_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "            mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "            mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "            post_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "            post_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "            with open(log_file, 'a') as file_log:\n",
    "                file_log.write(f\"               Failed to generate response for prompt with exception: {exception_name}\\n\") # 3 tabs\n",
    "                # write post-gen memory stats to the log file\n",
    "                file_log.write(f\"               Post-gen memory stats - allocated GPU0,GPU1 & reserved GPU0,GPU1: {post_gen_max_mem_allocated_gpu0}, {post_gen_max_mem_allocated_gpu1} & {post_gen_reserved_mem_gpu0}, {post_gen_reserved_mem_gpu1}\\n\") # 4 tabs\n",
    "            # experiment_results_df.loc[len(experiment_results_df)] = [experiment_name, PYTORCH_CUDA_ALLOC_CONF_value, prompt, input_ids.shape[-1], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, 0, 0, 0, 0, 0, \"\", status, exception_name, date_time]\n",
    "            experiment_results_df.loc[len(experiment_results_df)] = [experiment_name, PYTORCH_CUDA_ALLOC_CONF_value, prompt, input_ids.shape[-1], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer, status, exception_name , date_time]\n",
    "            experiment_results_df.to_csv(\"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/batch_experiment_results.csv\", index=False)\n",
    "            \n",
    "            \n",
    "            #del model\n",
    "            #gc.collect()\n",
    "            #torch.cuda.empty_cache()\n",
    "            #torch.cuda.reset_peak_memory_stats()\n",
    "            break\n",
    "        #except:\n",
    "         #   llm_answer = \"\"\n",
    "          #  end_time = time.time()\n",
    "           # status = \"Failed\"\n",
    "            #experiment_results_df.loc[len(experiment_results_df)] = [experiment_name, PYTORCH_CUDA_ALLOC_CONF_value, prompt, input_ids.shape[-1], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, 0, 0, 0, 0, 0, \"\", status,\"Other exception\", date_time]\n",
    "            #experiment_results_df.to_csv(\"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/batch_experiment_results.csv\", index=False)\n",
    "            #with open(log_file, 'a') as file_log:\n",
    "            #    file_log.write(f\"               Finished prompt with status: {status} & other exception\\n\") # 3 tabs\n",
    "            #print(\"             Failed with other exception: \") # 4 tabs\n",
    "        prompt_count += 1\n",
    "            \n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    print(\"     Model deleted for experiment: \", experiment_name)\n",
    "    with open(log_file, 'a') as file_log:\n",
    "            file_log.write(f\"       Model deleted for experiment: {experiment_name}\\n\") #2 tabs\n",
    "            file_log.write(f\"   Finished running experiment: {experiment_name}\\n\") # 1 tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "# experiment_results_df.to_csv(\"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/batch_experiment_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print rows 5-9 of the dataframe\n",
    "experiment_results_df[5:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi2_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
