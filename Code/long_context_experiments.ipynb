{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of experiments to run\n",
    "## 1] Default scenario\n",
    "## 2] flashattention\n",
    "## 3] expandable_segments: Set expandable segments -- check if it works in notebook\n",
    "## 4] flash_expandable: flashattention + expandable segments\n",
    "## 5] torch.compile\n",
    "## 6] sdpa\n",
    "## 7] sdpa_torch.compile: sdpa + torch.compile\n",
    "## 8] 8_bit: 8-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_for_experiment = [\"What is the firm period for Ognibene?\", \"What is the firm period for Hengli?\", \"What are the material planning requirements for Ognibene?\", \"What are the material planning requirements for Hengli?\", \"What are the warranty requirements for Hengli?\", \"Can you create a table showing the warranty requirements for Ognibene and Hengli?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  \n",
    "import torch\n",
    "from transformers.agents import ReactCodeAgent\n",
    "import pandas as pd\n",
    "import HF_TOKEN\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "from torch.nn.attention import SDPBackend, sdpa_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract_file_full = \"/home/vp899/projects/Agent_System/Input/Contracts/Full_Contracts_Consol.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(contract_file_full, 'r') as file_contract_full:\n",
    "    input_text_contract_full = file_contract_full.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_contract_asst = \"You are a helpful digital assistant. You will provide clear and concise answers on the input text you have been provided. You must answer in complete sentences. The input text is enclosed within <Input> and </Input>. The input text contains information on contracts with suppliers. Each individual supplier contract is enclosed within tags <Contract Between {Vendor Name} and CNH Industrial Italia SpA> and </Contract Between {Vendor Name} and CNH Industrial Italia SpA>. For example the contract information with Wipro Enterprises (P) Limited would be enclosed between the tags </Contract Between Wipro Enterprises (P) Limited and CNH Industrial Italia SpA> and </Contract Between Wipro Enterprises (P) Limited and CNH Industrial Italia SpA>. At the beginning of the contract text, there are also tags specifying the supplier name. \\n <Input> \\n\" +  input_text_contract_full + \"\\n </Input>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_chat = [{\"role\": \"system\", \"content\": system_prompt_contract_asst}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "llama31_hf_token = HF_TOKEN.HF_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the experiment to run\n",
    "# experiment_to_run = \"default\"\n",
    "experiment_to_run = \"expandable_segments\"\n",
    "experiment_to_run = \"sdpa\"\n",
    "experiment_to_run = \"torch.compile\"\n",
    "experiment_to_run = \"torch.compile_dynamic_mode\"\n",
    "experiment_to_run = \"torch.compile_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_to_run == \"expandable_segments\":\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# if experiment_to_run != default then set os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "if experiment_to_run != \"default\":\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the environment variable\n",
    "print(os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token = llama31_hf_token)\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_list = [\"flashattention_expandable\", \"torch.compile\", \"sdpa\", \"torch.compile_dynamic_mode\", \"sdpa_torch.compile\", \"8_bit\", \"8_bit_flash\"]\n",
    "experiment_list = [\"flashattention_expandable\", \"torch.compile\", \"sdpa\", \"sdpa_torch.compile\", \"8_bit\", \"8_bit_flash\"]\n",
    "experiment_list = [\"default\", \"expandable_segments\", \"flashattention_expandable\", \"8_bit\", \"8_bit_flash\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if experiment_to_run == \"default\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "if experiment_to_run == \"expandable_segments\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "if experiment_to_run == \"sdpa\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "if experiment_to_run == \"8_bit\":\n",
    "    quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token, attn_implementation=\"flash_attention_2\", quantization_config=quantization_config)\n",
    "if experiment_to_run == \"torch.compile\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "    model.generation_config.cache_implementation = \"static\"\n",
    "    model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "if experiment_to_run == \"torch.compile_dynamic_mode\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "    model.generation_config.cache_implementation = \"static\"\n",
    "    model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", dynamic=True)\n",
    "if experiment_to_run == \"torch.compile_model\":\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "    model.generation_config.cache_implementation = \"static\"\n",
    "    model = torch.compile(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with columns: Experiment_Name, Prompt, Context_Length, pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer\n",
    "experiment_results_df = pd.DataFrame(columns = [\"Experiment_Name\", \"Prompt\", \"Context_Length\", \"pre_gen_max_mem_allocated_gpu0\", \"pre_gen_max_mem_allocated_gpu1\", \"pre_gen_reserved_mem_gpu0\", \"pre_gen_reserved_mem_gpu1\", \"post_gen_max_mem_allocated_gpu0\", \"post_gen_max_mem_allocated_gpu1\", \"post_gen_reserved_mem_gpu0\", \"post_gen_reserved_mem_gpu1\", \"latency\", \"llm_answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_experiment = 1\n",
    "if batch_experiment == 1:\n",
    "    for experiment_name in experiment_list:\n",
    "        print(\"Running experiment: \", experiment_name)\n",
    "        if experiment_name == \"flashattention\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token, attn_implementation=\"flash_attention_2\",)\n",
    "        elif experiment_name == \"torch.compile\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "            model.generation_config.cache_implementation = \"static\"\n",
    "            model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "        elif experiment_name == \"sdpa\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "        elif experiment_name == \"sdpa_torch.compile\":\n",
    "            try:\n",
    "                model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "                model.generation_config.cache_implementation = \"static\"\n",
    "                model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "            except:\n",
    "                print(\"Error in running sdpa_torch.compile\")            \n",
    "        elif experiment_name == \"torch.compile_dynamic_mode\":\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,)\n",
    "            model.generation_config.cache_implementation = \"static\"\n",
    "            model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", dynamic=True)\n",
    "        elif experiment_to_run == \"8_bit\":\n",
    "            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token,  quantization_config=quantization_config)\n",
    "        elif experiment_to_run == \"8_bit_flash\":\n",
    "            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", token = llama31_hf_token, attn_implementation=\"flash_attention_2\", quantization_config=quantization_config)\n",
    "        print(\"     Model loaded for experiment: \", experiment_name)\n",
    "        for prompt in prompts_for_experiment:\n",
    "            # print experiment_name and prompt with prefix \"Running experiment: \" in a single line\n",
    "            print(\"             Running experiment: \", experiment_name, \" with prompt: \", prompt)\n",
    "\n",
    "            pre_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3) \n",
    "            pre_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "            mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "            mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "            pre_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "            # pre_gen_active_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"active_bytes.all.peak\"]/1024**3)\n",
    "            pre_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "            prompt_for_chat.append({\"role\": \"user\", \"content\": prompt})    \n",
    "            input_ids = tokenizer.apply_chat_template(prompt_for_chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "            start_time = time.time() \n",
    "            # if experiment name is sdpa or sdpa_torch.compile, use the sdpa_kernel context manager\n",
    "            if experiment_name == \"sdpa\" or experiment_name == \"sdpa_torch.compile\":\n",
    "                with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "                    outputs = model.generate(input_ids, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,)\n",
    "            else:\n",
    "                outputs = model.generate(input_ids, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,)\n",
    "            end_time = time.time()\n",
    "            latency = end_time - start_time\n",
    "            response =  outputs[0][input_ids.shape[-1]:]\n",
    "            llm_answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "            prompt_for_chat.append({\"role\": \"assistant\", \"content\": llm_answer})\n",
    "            post_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3)\n",
    "            post_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "            mem_stats_gpu0 = torch.cuda.memory_stats()\n",
    "            mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "            post_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "            post_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "            # add the information to the dataframe using loc\n",
    "            experiment_results_df.loc[len(experiment_results_df)] = [\"default\", prompt, input_ids.shape[-1], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer]\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "elif batch_experiment == 0:\n",
    "    prompts_for_experiment = [\"What is the firm period for Ognibene?\"]\n",
    "    # print the experiment_to_run with prefix \"Running experiment: \"\n",
    "    print(\"Running experiment: \", experiment_to_run)\n",
    "    # print experiment_to_run using {experiment_to_run}\n",
    "    \n",
    "\n",
    "    for prompt in prompts_for_experiment:\n",
    "        pre_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3) \n",
    "        pre_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "        mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "        mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "        pre_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        # pre_gen_active_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"active_bytes.all.peak\"]/1024**3)\n",
    "        pre_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        prompt_for_chat.append({\"role\": \"user\", \"content\": prompt})    \n",
    "        input_ids = tokenizer.apply_chat_template(prompt_for_chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "        start_time = time.time() \n",
    "        if experiment_to_run == \"sdpa\" or experiment_to_run == \"sdpa_torch.compile\":\n",
    "            with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "                outputs = model.generate(input_ids, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,)\n",
    "        else:\n",
    "                outputs = model.generate(input_ids, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,)  \n",
    "        # outputs = model.generate(input_ids, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,)\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        response =  outputs[0][input_ids.shape[-1]:]\n",
    "        llm_answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "        prompt_for_chat.append({\"role\": \"assistant\", \"content\": llm_answer})\n",
    "        post_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3)\n",
    "        post_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "        mem_stats_gpu0 = torch.cuda.memory_stats()\n",
    "        mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "        post_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        post_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "        # add the information to the dataframe using loc\n",
    "        experiment_results_df.loc[len(experiment_results_df)] = [\"default\", prompt, input_ids.shape[-1], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_df[\"llm_answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(experiment_to_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_for_experiment = [\"What is the firm period for Ognibene?\"]\n",
    "for prompt in prompts_for_experiment:\n",
    "    pre_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3) \n",
    "    pre_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "    mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "    mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "    pre_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "    # pre_gen_active_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"active_bytes.all.peak\"]/1024**3)\n",
    "    pre_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "    prompt_for_chat.append({\"role\": \"user\", \"content\": prompt})    \n",
    "    input_ids = tokenizer.apply_chat_template(prompt_for_chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    start_time = time.time()   \n",
    "    outputs = model.generate(input_ids, max_new_tokens=256, eos_token_id=terminators, do_sample=True, temperature=0.6, top_p=0.9,)\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "    response =  outputs[0][input_ids.shape[-1]:]\n",
    "    llm_answer = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    prompt_for_chat.append({\"role\": \"assistant\", \"content\": llm_answer})\n",
    "    post_gen_max_mem_allocated_gpu0 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(0)/1024**3)\n",
    "    post_gen_max_mem_allocated_gpu1 = \"{:.2f}\".format(torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "    mem_stats_gpu0 = torch.cuda.memory_stats()\n",
    "    mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "    post_gen_reserved_mem_gpu0 = \"{:.2f}\".format(mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "    post_gen_reserved_mem_gpu1 = \"{:.2f}\".format(mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "    # add the information to the dataframe using loc\n",
    "    experiment_results_df.loc[len(experiment_results_df)] = [\"default\", prompt, input_ids.shape[-1], pre_gen_max_mem_allocated_gpu0, pre_gen_max_mem_allocated_gpu1, pre_gen_reserved_mem_gpu0, pre_gen_reserved_mem_gpu1, post_gen_max_mem_allocated_gpu0, post_gen_max_mem_allocated_gpu1, post_gen_reserved_mem_gpu0, post_gen_reserved_mem_gpu1, latency, llm_answer]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "# experiment_results_df.to_csv(\"/home/vp899/projects/Long_Context_Chat/Long_Context_Chat/Output/experiment_results_expandable.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the main memory summary using torch.cuda.memory_summary()\n",
    "print(\"Before delete\")\n",
    "print(torch.cuda.memory_summary())\n",
    "\n",
    "# print maximum memory allocated and reserved memory for gpu0 and gpu1\n",
    "print(\"Maximum memory allocated for GPU0: \", torch.cuda.max_memory_allocated(0)/1024**3)\n",
    "print(\"Maximum memory allocated for GPU1: \", torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "print(\"Reserved memory for GPU0: \", mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "print(\"Reserved memory for GPU1: \", mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Main memory stats after delete\")\n",
    "print(torch.cuda.memory_summary(1))\n",
    "print(\"Maximum memory allocated for GPU0: \", torch.cuda.max_memory_allocated(0)/1024**3)\n",
    "print(\"Maximum memory allocated for GPU1: \", torch.cuda.max_memory_allocated(1)/1024**3)\n",
    "mem_stats_gpu0 = torch.cuda.memory_stats(0)\n",
    "mem_stats_gpu1 = torch.cuda.memory_stats(1)\n",
    "print(\"Reserved memory for GPU0: \", mem_stats_gpu0[\"reserved_bytes.all.peak\"]/1024**3)\n",
    "print(\"Reserved memory for GPU1: \", mem_stats_gpu1[\"reserved_bytes.all.peak\"]/1024**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pi4_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
