## Comparison of 2 huggingface transformer and pytorch versions
This GitHub repository contains code and results for experiments evaluating the performance of a large language model (LLM) for question-answering tasks with a long context. The experiments investigate the impact of different PyTorch & Transformer versions, FlashAttention, and 8-bit quantization on key performance metrics, including success rate, latency, response quality, and memory usage.
### Key Files:
1) Experiment_details.txt: This text file provides detailed information about the experiment setup, variables, and the columns in the results CSV file
2) Code/long_context_experiments_batch_errorcatch.ipynb: This Jupyter Notebook contains the code used to run the experiments.
3) Output/batch_experiment_results_20Dec_v2_GH.csv: This CSV file contains the raw results of the experiments.
4) Long_Context_Report_v2.pdf: Report summarizing the results of the experiment
